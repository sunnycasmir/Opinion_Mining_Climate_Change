{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1977b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libries\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as pty\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85324c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset and set the first row as the header\n",
    "df = pd.read_csv('ClimateChange.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aa5660d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Username</th>\n",
       "      <th>UTC Date</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Binders</th>\n",
       "      <th>Permalink</th>\n",
       "      <th>Retweet count</th>\n",
       "      <th>Likes count</th>\n",
       "      <th>Tweet value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shubhangi Wakodikar</td>\n",
       "      <td>@ShubhangiWakod1</td>\n",
       "      <td>6/8/2023 22:34:11</td>\n",
       "      <td>#SaveSoil #SaveSoilMovement\\n@cpsavesoil\\n@Sad...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.twitter.com/user/status/1688317574...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joy finds hope</td>\n",
       "      <td>@joyfindshope</td>\n",
       "      <td>6/8/2023 22:34:01</td>\n",
       "      <td>Yes, it's good news. We are on the right track...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.twitter.com/user/status/1688317531...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shubhangi Wakodikar</td>\n",
       "      <td>@ShubhangiWakod1</td>\n",
       "      <td>6/8/2023 22:33:46</td>\n",
       "      <td>#SaveSoil #SaveSoilMovement\\n@cpsavesoil\\n@Sad...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.twitter.com/user/status/1688317466...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Daniel Davison</td>\n",
       "      <td>@ClimateClamor</td>\n",
       "      <td>6/8/2023 22:29:44</td>\n",
       "      <td>Reductions in SO2 emissions from container shi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.twitter.com/user/status/1688316454...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Peter D Carter</td>\n",
       "      <td>@PCarterClimate</td>\n",
       "      <td>6/8/2023 22:26:04</td>\n",
       "      <td>RECORD HEAT KILLS CORALS FLORIDA GULF MEXICO\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.twitter.com/user/status/1688315529...</td>\n",
       "      <td>39</td>\n",
       "      <td>47</td>\n",
       "      <td>93.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35002</th>\n",
       "      <td>Shubhangi Wakodikar</td>\n",
       "      <td>@ShubhangiWakod1</td>\n",
       "      <td>6/8/2023 22:35:14</td>\n",
       "      <td>#SaveSoil #SaveSoilMovement\\n@cpsavesoil\\n@Sad...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.twitter.com/user/status/1688317838...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35003</th>\n",
       "      <td>Shubhangi Wakodikar</td>\n",
       "      <td>@ShubhangiWakod1</td>\n",
       "      <td>6/8/2023 22:34:57</td>\n",
       "      <td>@ishavidhya #SaveSoil #SaveSoilMovement\\n@cpsa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.twitter.com/user/status/1688317765...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35004</th>\n",
       "      <td>Shubhangi Wakodikar</td>\n",
       "      <td>@ShubhangiWakod1</td>\n",
       "      <td>6/8/2023 22:34:49</td>\n",
       "      <td>#SaveSoil #SaveSoilMovement\\n@cpsavesoil\\n@Sad...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.twitter.com/user/status/1688317732...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35005</th>\n",
       "      <td>Shubhangi Wakodikar</td>\n",
       "      <td>@ShubhangiWakod1</td>\n",
       "      <td>6/8/2023 22:34:31</td>\n",
       "      <td>@SadhguruJV #SaveSoil #SaveSoilMovement\\n@cpsa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.twitter.com/user/status/1688317656...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35006</th>\n",
       "      <td>Shubhangi Wakodikar</td>\n",
       "      <td>@ShubhangiWakod1</td>\n",
       "      <td>6/8/2023 22:34:21</td>\n",
       "      <td>@ishafoundation #SaveSoil #SaveSoilMovement\\n@...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.twitter.com/user/status/1688317616...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35007 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      User          Username           UTC Date  \\\n",
       "0      Shubhangi Wakodikar  @ShubhangiWakod1  6/8/2023 22:34:11   \n",
       "1           Joy finds hope     @joyfindshope  6/8/2023 22:34:01   \n",
       "2      Shubhangi Wakodikar  @ShubhangiWakod1  6/8/2023 22:33:46   \n",
       "3           Daniel Davison    @ClimateClamor  6/8/2023 22:29:44   \n",
       "4           Peter D Carter   @PCarterClimate  6/8/2023 22:26:04   \n",
       "...                    ...               ...                ...   \n",
       "35002  Shubhangi Wakodikar  @ShubhangiWakod1  6/8/2023 22:35:14   \n",
       "35003  Shubhangi Wakodikar  @ShubhangiWakod1  6/8/2023 22:34:57   \n",
       "35004  Shubhangi Wakodikar  @ShubhangiWakod1  6/8/2023 22:34:49   \n",
       "35005  Shubhangi Wakodikar  @ShubhangiWakod1  6/8/2023 22:34:31   \n",
       "35006  Shubhangi Wakodikar  @ShubhangiWakod1  6/8/2023 22:34:21   \n",
       "\n",
       "                                                   Tweet  Binders  \\\n",
       "0      #SaveSoil #SaveSoilMovement\\n@cpsavesoil\\n@Sad...      NaN   \n",
       "1      Yes, it's good news. We are on the right track...      NaN   \n",
       "2      #SaveSoil #SaveSoilMovement\\n@cpsavesoil\\n@Sad...      NaN   \n",
       "3      Reductions in SO2 emissions from container shi...      NaN   \n",
       "4      RECORD HEAT KILLS CORALS FLORIDA GULF MEXICO\\n...      NaN   \n",
       "...                                                  ...      ...   \n",
       "35002  #SaveSoil #SaveSoilMovement\\n@cpsavesoil\\n@Sad...      NaN   \n",
       "35003  @ishavidhya #SaveSoil #SaveSoilMovement\\n@cpsa...      NaN   \n",
       "35004  #SaveSoil #SaveSoilMovement\\n@cpsavesoil\\n@Sad...      NaN   \n",
       "35005  @SadhguruJV #SaveSoil #SaveSoilMovement\\n@cpsa...      NaN   \n",
       "35006  @ishafoundation #SaveSoil #SaveSoilMovement\\n@...      NaN   \n",
       "\n",
       "                                               Permalink  Retweet count  \\\n",
       "0      https://www.twitter.com/user/status/1688317574...              0   \n",
       "1      https://www.twitter.com/user/status/1688317531...              0   \n",
       "2      https://www.twitter.com/user/status/1688317466...              0   \n",
       "3      https://www.twitter.com/user/status/1688316454...              0   \n",
       "4      https://www.twitter.com/user/status/1688315529...             39   \n",
       "...                                                  ...            ...   \n",
       "35002  https://www.twitter.com/user/status/1688317838...              0   \n",
       "35003  https://www.twitter.com/user/status/1688317765...              0   \n",
       "35004  https://www.twitter.com/user/status/1688317732...              0   \n",
       "35005  https://www.twitter.com/user/status/1688317656...              0   \n",
       "35006  https://www.twitter.com/user/status/1688317616...              0   \n",
       "\n",
       "       Likes count  Tweet value  \n",
       "0                0         0.18  \n",
       "1                0         0.03  \n",
       "2                0         0.18  \n",
       "3                0         0.01  \n",
       "4               47        93.24  \n",
       "...            ...          ...  \n",
       "35002            0         0.18  \n",
       "35003            0         0.03  \n",
       "35004            0         0.18  \n",
       "35005            0         0.03  \n",
       "35006            0         0.03  \n",
       "\n",
       "[35007 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95be1c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "  analysis = TextBlob(text)\n",
    "  \n",
    "  if analysis.sentiment.polarity > 0: \n",
    "    return 'positive'\n",
    "\n",
    "  else:\n",
    "    return 'negative'\n",
    "  \n",
    "df['Sentiment'] = df['Tweet'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a9a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the DataFrame to view the result\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c9d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the two columns needed for my analysis\n",
    "df_new = df[['Tweet', 'Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c6a674",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b2d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplication in my dataset\n",
    "\n",
    "df_new.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198db4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplication\n",
    "\n",
    "df_new.drop_duplicates(keep ='first', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c64202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check\n",
    "df_new.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50080084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for NAN values\n",
    "df_new['Sentiment'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['Sentiment'] = df_new['Sentiment'].replace({'negative': 0,'positive': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f37ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity checks \n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d98b7",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8794bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def wordcloud_create(data_df, sentvalue):\n",
    "    reviews = df_new[df_new['Sentiment'] == sentvalue]\n",
    "    words = ' '.join(reviews['Tweet']) \n",
    "\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                          background_color='black',\n",
    "                          width=2995,\n",
    "                          height=2510).generate(words)\n",
    "\n",
    "    plt.figure(1, figsize=(13, 13))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d76ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the negative reviews\n",
    "wordcloud_create(df_new, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5676f43c",
   "metadata": {},
   "source": [
    "- Some of the negative words observed are flood, less, extreme weather, cause and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3270f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the positive reviews\n",
    "wordcloud_create(df_new, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aecd526",
   "metadata": {},
   "source": [
    "- Some positive words are sustainability, great, protect and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98187f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the labels of my dataframe\n",
    "df_new['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa12bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the distribution of the labelled class\n",
    "\n",
    "def plot_bar(df_new, feature):\n",
    "    plot = sns.countplot(x=feature, data=df_new)\n",
    "\n",
    "    # calculating the length of the total data\n",
    "    total_data = len(df_new)\n",
    "\n",
    "    # Display the percentage on top of each bar\n",
    "    for p in plot.patches:\n",
    "        height = p.get_height()\n",
    "        plot.text(p.get_x() + p.get_width() / 2., height + 3, f'{height / total_data:.2%}', ha=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0ff322",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar(df_new, 'Sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70bc8f",
   "metadata": {},
   "source": [
    "- The variation orr difference between the classes are not much hence there will be no need for data balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6140eeef",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e403b55f",
   "metadata": {},
   "source": [
    "#### HTML Tags Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define a function to remove HTML tags from text\n",
    "def remove_html_tags(html_text):\n",
    "    if isinstance(html_text, str):  # Check if the element is a string\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "        return text\n",
    "    else:\n",
    "        return html_text  # Return the original value if it's not a string\n",
    "\n",
    "\n",
    "\n",
    "# Apply the remove_html_tags function to the 'Sentiment' column\n",
    "df_new['Tweet'] = df_new['Tweet'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279649a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d4127d",
   "metadata": {},
   "source": [
    "#### Fix Contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac49ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "# Define a function to apply the contraction fix\n",
    "def fix_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# Apply the contraction fix to the 'review' column\n",
    "df_new['Tweet'] = df_new['Tweet'].apply(fix_contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407ff01",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "df_new['Tweet'] = df_new.apply(lambda row: nltk.word_tokenize(row['Tweet']), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b889270",
   "metadata": {},
   "source": [
    "#### remove the Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f500be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_remove(row):\n",
    "    tokens = [word for word in row if not word.isdigit()]\n",
    "    return tokens\n",
    "df_new['Tweet'] = df_new['Tweet'].apply(lambda x: digit_remove(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e7f737",
   "metadata": {},
   "source": [
    "#### Remove Non-Ascii Character "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c0153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def non_ascii_removal(row):\n",
    "    row_new = []\n",
    "    for word in row:\n",
    "        word_new = unicodedata.normalize('NFKD', word).encode('ascii','ignore').decode('utf-8', 'ignore')\n",
    "        row_new.append(word_new)\n",
    "    return row_new\n",
    "df_new['Tweet'] = df_new['Tweet'].apply(lambda x: non_ascii_removal(x))\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc6d737",
   "metadata": {},
   "source": [
    "#### Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7426af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def removeRuntuaction(row):\n",
    "    tokens = [word for word in row if word not in string.punctuation]\n",
    "    return tokens\n",
    "df_new['Tweet'] = df_new['Tweet'].apply(lambda x: removeRuntuaction (x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2836f",
   "metadata": {},
   "source": [
    "#### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae39e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(row):\n",
    "    words_rows = [word.lower()for word in row]\n",
    "    return words_rows\n",
    "df_new['Tweet'] = df_new['Tweet'].apply(lambda x: lowercase (x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fda160",
   "metadata": {},
   "source": [
    "#### Stopwords removals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f140c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "words_stop = stopwords.words('english')\n",
    "\n",
    "def removestopwords(row):\n",
    "    words = [word for word in row if word not in words_stop]\n",
    "    return words\n",
    "df_new['Tweet'] = df_new['Tweet'].apply(lambda x: removestopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb69409",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8508945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def words_lemmatize(row):\n",
    "    words = [lem.lemmatize(word) for word in row]\n",
    "    return words\n",
    "df_new['Tweet'] = df_new['Tweet'].apply(lambda x: words_lemmatize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c2a607",
   "metadata": {},
   "source": [
    "#### Regular Expression Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e5b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def word_join(row):\n",
    "    words = ' '.join([word for word in row])\n",
    "    words = re.sub('[^a-zA-Z]', ' ', words)  # Changed '2' to 'Z' in the regex pattern\n",
    "    return words  # Return the modified 'words' variable\n",
    "\n",
    "df_new['Tweet'] = df_new['Tweet'].apply(lambda x: word_join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a672ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the first 5 rows to be sure that the data is clean\n",
    "df_new.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e350216",
   "metadata": {},
   "source": [
    "#### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3904cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorization (text conversion to numbers)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Keep only 500 words\n",
    "vec_count = CountVectorizer(max_features=500)\n",
    "\n",
    "result_df = vec_count.fit_transform(df_new['Tweet'])\n",
    "\n",
    "#feature conversion to arrays\n",
    "\n",
    "result_df = result_df.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496f296",
   "metadata": {},
   "source": [
    "#### Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bb7cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = result_df\n",
    "y = df_new['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539deef6",
   "metadata": {},
   "source": [
    "#### Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2199532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting of data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c178325a",
   "metadata": {},
   "source": [
    "#### Random Forest on Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04338684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 0, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97609b8",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a91ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62994aa",
   "metadata": {},
   "source": [
    "#### Confussion Metrix and Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7466c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def score_metrics(actual, predicted):\n",
    "    labels = ['negative','positive']  \n",
    "    print(classification_report(actual, predicted, target_names=labels))\n",
    "    cm = confusion_matrix(actual, predicted)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c73e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction\n",
    "\n",
    "y_pred = rf.predict(x_test)\n",
    "\n",
    "#check the metrics\n",
    "\n",
    "score_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8175b4c7",
   "metadata": {},
   "source": [
    "#### Randomforest with Balanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a279a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import SMOTE library\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e963043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state = 0) #instance of SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd55a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature and target performance\n",
    "x_sm,y_sm = sm.fit_resample(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28899031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data spliting into training and testing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split the data\n",
    "x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(x_sm,y_sm, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e26fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_1 = RandomForestClassifier(random_state = 0, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8debde7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_1.fit(x_train_1, y_train_1) #train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e874e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions\n",
    "y_pred_1 = rf_1.predict(x_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bcb3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_metrics(y_test_1, y_pred_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199de072",
   "metadata": {},
   "source": [
    "- The model with unbalanced data performed better that performed better hence will continue with the unbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c167b7",
   "metadata": {},
   "source": [
    "#### Word vetctorization instatiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2970f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_top50_words(model, all_features):\n",
    "    topfeature = ''\n",
    "    feat = model.feature_importances_\n",
    "    feature = sorted(range(len(feat)), key=lambda k: feat[k], reverse=True)[:50]\n",
    "    for i in feature:\n",
    "        topfeature += all_features[i]\n",
    "        topfeature += ' '\n",
    "    \n",
    "    wordcloud = WordCloud(background_color='white', colormap='viridis', width=800, height=400).generate(topfeature)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title('Top 50 Featured wordcloud')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the appropriate parameters\n",
    "features = vec_count.get_feature_names_out()\n",
    "get_top50_words(rf, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d086f24a",
   "metadata": {},
   "source": [
    "- Some of the words observed are action, new, great, change, better, important, weather, clean,heat, good, latest, first, sustainability and others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af249f8",
   "metadata": {},
   "source": [
    "#### Barchat plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179950b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top50_words(model, all_features):\n",
    "    feat_importance = model.feature_importances_\n",
    "    sorted_idx = feat_importance.argsort()[::-1]\n",
    "    \n",
    "    top_features = [all_features[idx] for idx in sorted_idx[:50]]\n",
    "    top_importances = feat_importance[sorted_idx][:50]\n",
    "\n",
    "    return top_features, top_importances\n",
    "\n",
    "features = vec_count.get_feature_names_out()\n",
    "top_features, top_importances = get_top50_words(rf, features)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(top_features, top_importances)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Top 50 Important Words')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b194a786",
   "metadata": {},
   "source": [
    "- Words like new, action, climate and great contributes more. we also have other words like important, better ,latest and other s that contributed to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8bc568",
   "metadata": {},
   "source": [
    "#### GradientBoosting on Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba405807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the library\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b6e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d333ac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb.fit(x_train, y_train) # train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c546f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred_gb = gb.predict(x_test)\n",
    "\n",
    "score_metrics(y_test,y_pred_gb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbf3396",
   "metadata": {},
   "source": [
    "- The model with random forest on bag of words performed better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5f1eeb",
   "metadata": {},
   "source": [
    "#### RandoModel with Pretrained word2vec embeddings - SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ecc8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract out sentences from the data\n",
    "list_word = [sentence.split() for sentence in df_new['Tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b50fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of word2vec\n",
    "#import word2vec libraries\n",
    "from gensim.models import Word2Vec\n",
    "word2vec_model = Word2Vec(list_word, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b172e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the vocabularies\n",
    "vocab_1 = word2vec_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b988b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the lenght of words\n",
    "len(vocab_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c33311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top Similar by words 'great'\n",
    "word2vec_model.wv.similar_by_word('climate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae06d43e",
   "metadata": {},
   "source": [
    "- Change, inevitatble, physical, respinse and others shown above are the words trained together with climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dfd6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vectorizer(model, corpus):\n",
    "    vocab = set(model.wv.key_to_index)  # extract unique words in the vocabulary \n",
    "    word2vec_features = [getwordvec_embeddings(model, tokenized_sentence, vocab) for tokenized_sentence in corpus]\n",
    "    return np.array(word2vec_features)\n",
    "\n",
    "def getwordvec_embeddings(model, words, vocab):\n",
    "    embeddings = []\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            embeddings.append(model.wv[word])  # Use model.wv to access word vectors\n",
    "    if len(embeddings) == 0:\n",
    "        # If none of the words in the sentence are in the model's vocabulary, return zeros\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        # Return the mean of word embeddings for the words in the vocabulary\n",
    "        return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c834ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_array = word_vectorizer(model = word2vec_model , corpus = list_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc50e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234a4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(feature_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ccf45",
   "metadata": {},
   "source": [
    "- These are the word2vwc embedinngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bd0c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "x_word2vec = feature_array\n",
    "y_word2vec = df_new['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split my data into training and testing\n",
    "\n",
    "x_train_word2vec, X_text_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(x_word2vec, y_word2vec, test_size = 0.30, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5af628",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_rf = RandomForestClassifier(random_state = 0) #create and instance of random forest\n",
    "\n",
    "word2vec_rf.fit(x_train_word2vec, y_train_word2vec) #train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f6ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction\n",
    "y_pred_word2vec = word2vec_rf.predict(X_text_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b7ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assees model performance\n",
    "score_metrics(y_test_word2vec, y_pred_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82202ed",
   "metadata": {},
   "source": [
    "- The result is not better when compared with Randoom and Gradientboosting on Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce365774",
   "metadata": {},
   "source": [
    "#### RandomForest with Continue Bags of words (CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of word2vec with CBOW\n",
    "model_word2vec_cbow = Word2Vec(list_word, sg = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7da7678",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_array_cbow = word_vectorizer(model = model_word2vec_cbow , corpus = list_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b6b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "x_word2vec_cbow = feature_array_cbow\n",
    "y_word2vec_cbow = df_new['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963692bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split my data into training and testing\n",
    "\n",
    "x_train_cbow, X_text_cbow, y_train_cbow, y_test_cbow = train_test_split(x_word2vec_cbow, y_word2vec_cbow, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa302faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#create an instance of random forest for cbow\n",
    "cbow_rf = RandomForestClassifier(random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd04e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "cbow_rf.fit(x_train_cbow, y_train_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e9062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction on the test\n",
    "y_pred_cbow = cbow_rf.predict(X_text_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e5639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation metrics\n",
    "score_metrics(y_test_cbow, y_pred_cbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c1ec71",
   "metadata": {},
   "source": [
    "- The accuracy here is 69 which is a poor result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7995bed1",
   "metadata": {},
   "source": [
    "#### GradientBoosting - Skip Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of word2vec \n",
    "model_word2vec_sg = Word2Vec(list_word, sg = 1)\n",
    "feature_array_sg = word_vectorizer(model = model_word2vec_sg, corpus = list_word )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c94ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "x_word2vec_sg = feature_array_sg\n",
    "y_word2vec_sg = df_new['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b220388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split my data into training and testing\n",
    "x_train_sg, X_text_sg, y_train_sg, y_test_sg = train_test_split(x_word2vec_sg, y_word2vec_sg, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0f5dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the library\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#create and instance of Gradient Boosting\n",
    "word2vec_gb_sg = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "#train the model\n",
    "word2vec_gb_sg.fit(x_train_sg, y_train_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3687521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction on the test\n",
    "y_pred_gb_sg = word2vec_gb_sg.predict(X_text_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation metrics\n",
    "\n",
    "score_metrics(y_test_sg, y_pred_gb_sg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6676281c",
   "metadata": {},
   "source": [
    "- Still a poor result when compared with BOWs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c613d",
   "metadata": {},
   "source": [
    "#### GradientBoosting - Continuous Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ac48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of word2vec with CBOW\n",
    "model_word2vec_cbow_gb = Word2Vec(list_word, sg = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71cd832",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_array_cbow_gb = word_vectorizer(model = model_word2vec_cbow_gb , corpus = list_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd78c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "x_word2vec_cbow_gb = feature_array_cbow_gb\n",
    "y_word2vec_cbow_gb = df_new['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4730cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split my data into training and testing\n",
    "x_train_cbow_gb, X_text_cbow_gb, y_train_cbow_gb, y_test_cbow_gb = train_test_split(x_word2vec_cbow_gb, y_word2vec_cbow_gb, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and instance of Gradient Boosting\n",
    "cbow_gb = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "#train the model\n",
    "cbow_gb.fit(x_train_cbow_gb, y_train_cbow_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction on the test\n",
    "y_pred_cbow_gb = cbow_gb.predict(X_text_cbow_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93666c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation metrics\n",
    "score_metrics(y_test_cbow_gb, y_pred_cbow_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c4601a",
   "metadata": {},
   "source": [
    "- At 65% accuracy, the modle did not do well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd5827",
   "metadata": {},
   "source": [
    "#### Deep Learning - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8135e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required packages\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, SimpleRNN, LSTM, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10e2cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000) # tokenize to convert to numericals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad9580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit my tokenizer on texts\n",
    "tokenizer.fit_on_texts(df_new['Tweet'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66acd003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to extract the texts in sequence\n",
    "x = tokenizer.texts_to_sequences(df_new['Tweet'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc32b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#path to save my tokenizer which can be used during mo\n",
    "with open(r'C:\\Users\\User\\Videos\\Obruche\\tokens.pk1', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56973cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the lenght of row 200000 for just sanity checks\n",
    "len(x[25000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce44d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6449a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code that tokenize and count the lenght\n",
    "\n",
    "def sentencecount(row):\n",
    "    tokens = word_tokenize(row)\n",
    "    lenght = len(tokens)\n",
    "    return lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce244a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['sentence_count'] = df_new['Tweet'].apply(lambda x: sentencecount(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[df_new['sentence_count'] < 25].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b5aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[df_new['sentence_count'] > 25].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d28cec",
   "metadata": {},
   "source": [
    "- 25 seems to be a better trade off point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ae150",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pad_sequences(x, maxlen = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66de421",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19119813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the label\n",
    "y= df_new['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24cf708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "x_train, x_test, y_train,  y_test = train_test_split(x,y, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d5fe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Create a LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the training labels\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Transform the test labels\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Reshape the encoded labels to a 2D array\n",
    "y_train_encoded_reshaped = y_train_encoded.reshape(-1, 1)\n",
    "y_test_encoded_reshaped = y_test_encoded.reshape(-1, 1)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train_cat = onehot_encoder.fit_transform(y_train_encoded_reshaped)\n",
    "y_test_cat = onehot_encoder.transform(y_test_encoded_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402686f6",
   "metadata": {},
   "source": [
    "#### Build LSTM Architecture Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a0f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, SpatialDropout1D, Embedding, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Build the LSTM Architecture\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(10000, 150, input_length=25))\n",
    "model_lstm.add(SpatialDropout1D(0.3))\n",
    "model_lstm.add(LSTM(100))\n",
    "model_lstm.add(Dropout(0.6))  # 60% dropout\n",
    "model_lstm.add(Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42acb25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the mode\n",
    "model_lstm.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97db940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Specify the directory\n",
    "save_directory_lstm = r'C:\\Users\\User\\Videos\\Obruche\\lstm_model'\n",
    "\n",
    "# Specify the filename for your model\n",
    "model_filename_lstm = \"lstm_model.h5\"\n",
    "\n",
    "# Create the absolute file path using os.path.join\n",
    "path_lstm = os.path.join(save_directory_lstm, model_filename_lstm)\n",
    "\n",
    "\n",
    "#path_lstm = \"Sentiment_analysis\\LSTM_Models\\lstm.hdf5\"\n",
    "es_lstm = EarlyStopping(patience = 10, verbose = 1)\n",
    "chkpt_lstm = ModelCheckpoint(path_lstm, verbose = 1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de2057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "history_lstm = model_lstm.fit(x_train,y_train_cat, batch_size = 32, epochs = 10, validation_split=0.2, callbacks=[es_lstm, chkpt_lstm])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7780157",
   "metadata": {},
   "source": [
    "#### Tuned LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e8c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, LSTM\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Tuned LSTM Architecture with Regularization\n",
    "model_lstm_2 = Sequential()\n",
    "model_lstm_2.add(Embedding(10000, 128, input_length=x.shape[1])) #Reduced embedding dimension from 150 to 128\n",
    "model_lstm_2.add(SpatialDropout1D(0.4)) # Reduced dropout rate from 0.5 \n",
    "model_lstm_2.add(LSTM(64, kernel_regularizer=l2(0.01))) # Reduced LSTM units from 100 to 64, L2 regularization of 0.01\n",
    "model_lstm_2.add(Dropout(0.3)) #Reduced dropout rate from 0.4\n",
    "model_lstm_2.add(Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e63eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the mode\n",
    "model_lstm_2.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "model_lstm_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6364e44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Specify the directory\n",
    "save_directory_lstm_2 = r'selected_lstm'\n",
    "\n",
    "# Specify the filename for your model\n",
    "model_filename_lstm_2 = \"lstm_model.h5\"\n",
    "\n",
    "# Create the absolute file path using os.path.join\n",
    "path_lstm_2 = os.path.join(save_directory_lstm_2, model_filename_lstm_2)\n",
    "\n",
    "\n",
    "#path_lstm = \"Sentiment_analysis\\LSTM_Models\\lstm.hdf5\"\n",
    "es_lstm_2 = EarlyStopping(patience = 2, verbose = 1) \n",
    "chkpt_lstm_2 = ModelCheckpoint(path_lstm_2, verbose = 1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a873e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "history_lstm_2 = model_lstm_2.fit(x_train,y_train_cat, batch_size = 32, epochs = 10, validation_split=0.2, callbacks=[es_lstm_2, chkpt_lstm_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f1f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation loss \n",
    "plt.plot(history_lstm_2.history['loss'])\n",
    "plt.plot(history_lstm_2.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d81037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation accuracy\n",
    "plt.plot(history_lstm_2.history['accuracy'])\n",
    "plt.plot(history_lstm_2.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e533a3e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e9f80a1",
   "metadata": {},
   "source": [
    "#### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c04af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SpatialDropout1D, LSTM, Bidirectional, Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model_bilstm = Sequential()\n",
    "# Input embedding layer\n",
    "model_bilstm.add(Embedding(10000, 128, input_length=x.shape[1]))\n",
    "# Spatial 1D dropout for regularization\n",
    "model_bilstm.add(SpatialDropout1D(0.4))  \n",
    "# Bidirectional LSTM layer\n",
    "model_bilstm.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "# Dropout between LSTM layers for regularization\n",
    "model_bilstm.add(Dropout(0.3))\n",
    "# Second bidirectional LSTM layer \n",
    "model_bilstm.add(Bidirectional(LSTM(32)))\n",
    "# Output layer \n",
    "model_bilstm.add(Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e54d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_bilstm.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4cac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bilstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51774e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "history_bilstm_2 = model_bilstm.fit(x_train,y_train_cat, batch_size = 32, epochs = 10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47dd68e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
